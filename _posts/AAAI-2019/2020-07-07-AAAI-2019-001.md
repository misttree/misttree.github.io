---
layout: post
title: "Multi-Task Learning with Multi-View Attention for
Answer Selection and Knowledge Base Question Answering"
date: 2020-07-07 20:00:00 +0800 
categories: AAAI-2019
tags: AAAI-2019

---
* content
{:toc}
---

<!-- more -->

# Conference： AAAI-2019

## Multi-Task Learning with Multi-View Attention for Answer Selection and Knowledge Base Question Answering

答案选择（Answer Selection）与基于知识库的问答(Knowledge Base question answer)是问答系统中的两个重要的任务，本文通过使用多任务机制来将两个任务的进行联合处理。该想法主要来自于以下两点

1. 两个任务都可以理解为一种排序问题，一个是在文本层次上进行排序，另一个是在知识库的层次上进行排序
2. 两个任务之间的可以相互促进，基于知识库的问答可以通过学习文本性的内容得到提升，而答案选择可以通过吸收知识库的内容提高模型的效果

为了更好的体现联合训练的效果，我们提出了一种新的多视角的注意力机制，通过从不同视角进行数据交互来得到对于句子的更加明确的表示
最终通过在多个真实数据集上进行实验，我们证明了所提出方法的高效性，并且通过设置独立实验验证了我们所提出的多视角的注意力机制所具有的高效性

*** 

### Multi-Task Learning for Question Answering

答案选择与基于知识库的问答可以看作是一个排序问题，我们通过给出一个问题q以及一系列的答案$$a_i ϵ A$$,我们分别计算f(q,a)得到一个相关性的概率，而概率最高值即可作为问题的答案，而这种多任务的学习模式可以参考下图：

![table 1](https://s1.ax1x.com/2020/07/07/UASieH.jpg)

输入为一段单词序列与一个知识库序列内容，对于答案选择中的问题与答案以及基于知识库的回答中的问题，我们使用实体连接（Entity Linking）来提取其中的知识部分，对于基于知识库的问答中，我们通过分析标记化的实体名称与关系名称来获取词序列。

而模型的整体结构可以参考下图形式：

![model 1](https://s1.ax1x.com/2020/07/07/UApeE9.jpg)

模型的整体结构为一个深度神经网络结构，并且采用了分层的共享机制，在一些较高的层上进行了信息的共享，而针对于其他的部分都是相互平行并且独立的。下面会分层进行介绍：

1. Task-specific Encoder Layer：

预处理的句子会首先被编码为一个向量表示，不同的问答任务应该在数据分布与低维度的数据表征上具有不同的内容，因此每一个任务我们配备了独立的编码器结构，并且每一个编码器结构都包含有一个词编码器与一个知识库编码器。整体可以见下图:

![model 2](https://s1.ax1x.com/2020/07/08/UVuXbd.jpg)

为了充分利用句子中所包含的信息，我们使用一个双向的LSTM编码器针对于句子中的问题与答案进行编码处理，给定一组问题与答案的语句表示，生成基于词向量的语句表征。

与词向量的编码不同，知识库序列的编码需要通过另一种方式进行，因为知识库的信息大多都是一种标记化的实体或关系信息，我们通过引入卷积神经网络来进行，在知识序列上，大小为n的过滤器在知识嵌入矩阵上滑动，获取局部的n-gram特征。每次移动并计算一个隐藏层向量。最终得到整体的编码格式。

由于针对于词向量的编码以及针对于知识库信息的编码都是顺序相关的，所以我们可以使用连接层将两者的信息进行连接得到最终的语义表征向量

$$H_q = [H_W_q:H_K_q], H_a = [H_W_a:H_K_a]$$

2. Shared Representation Learning Layer:

在通过特定任务编码器将句子编码成为了特定的表征向量后，我们在较高的维度将不同的任务之间的信息通过一个共享表征学习层进行交互。与基于任务的编码器的输入相比较，共享表征学习层的输入数据包含有更为丰富的语义含义，并且与其他的任务之间共享了更多的具有相似分布的信息。因此我们使用一个双向的LSTM针对于共享信息进行处理，并分别针对于问题与答案进行编码处理。而后我们使用一个，平均池来对于LSTM的输出进行处理。

受到前人研究的启发，我们引入了一些词向量与知识库的重叠特征值来构造最终的特征空间，主要包括：单词重叠分、不间断单词重叠分、加权单词重叠分、不间断加权单词重叠分、知识重叠分、加权知识重叠分。

3. Task-specific Softmax Layer

在某一个特定的任务中，对于一个问题回答对，他的问题，答案的编码信息，以及标签信息，最后都会灌入特定任务的softmax层中来进行二进制的分类，分类函数如下：

$$p^{(t)}=softmax(W_s^{(t)}x+b_s^{(t)})$$

4. Multi-Task Learning 

以上的多任务学习模型会通过最小化交叉熵损失来进行训练，损失函数为：

$$L=-∑_{t=1}^Tλ_t∑_{i=1}^{N_t}[y_i^tlogp_i^t+(1-y_i^t)log(1-p_i^t)]$$

其中λ是决定了某一个任务的重要性的权重

--- 

### Multi-Task Model with Multi-View Attention

为了增加不同的QA任务在隐表征空间上的交互，我们提出了多视角的注意力机制，来捕获特定任务层以及共享层的重要信息。

如下图所示，我们从多个角度对于注意力信息进行获取，包括词向量层，知识库层，以及词向量语义层与知识库语义层，以及联合注意力机制，并最终对于多个注意力机制所得到的注意力信息进行相加得到最终的注意力结果

![model 3](https://s1.ax1x.com/2020/07/08/UVGpwD.jpg)

